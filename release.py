# -*- coding: utf-8 -*-

from bs4 import BeautifulSoup
from pathlib import Path
import shutil
import subprocess

enc = 'utf-8'
parser = 'html.parser'

release=Path('release')
first_src=Path('chipinfo')
second_src=Path('microshemca')

def readhtml(s: str):
    return BeautifulSoup(Path(s).read_text(encoding = enc), parser)

def savehtml(html, s: str):
    html.smooth()
    Path(s).write_text(html.prettify(), enc)

import time
start_time = time.time()
end_time = time.time()

def log(msg: str):
    global start_time, end_time
    end_time = time.time()
    elaps = end_time - start_time
    print(f' ({elaps:.2f}).')
    print(msg, end = '')
    start_time = time.time()

class Sitemap(list[str]):
    __parser = 'html.parser' # 'xml'
    def __init__(self, fname: str):
        self.__xml = BeautifulSoup(Path(fname).read_text('utf-8'), self.__parser)
        super().__init__()

    def save(self, fname: str, url: str):
        from datetime import datetime
        self.__lastmod = datetime.today().strftime('%Y-%m-%d')
        self.__changefreq = 'never'
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–∞—Ä—Ç—É
        self.__xml.urlset.extend([BeautifulSoup(f'<url><loc>https://{url}/{i}</loc><lastmod>{self.__lastmod}</lastmod><changefreq>{self.__changefreq}</changefreq></url>', self.__parser) for i in self])
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        for i in self.__xml.urlset:
            imgs = {}
            for img in readhtml(i.loc.text.strip().replace(f'https://{url}/', '')).find_all('img'):
                imgs[img['src']] = img.get('alt', None)
            for k, v in imgs.items():
                if v:
                    im = BeautifulSoup(f'<image:image><image:loc>https://{url}/{k}</image:loc><image:title>{v}</image:title></image:image>', self.__parser)
                else:
                    im = BeautifulSoup(f'<image:image><image:loc>https://{url}/{k}</image:loc></image:image>', self.__parser)
                i.loc.insert_after(im)
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞—Ä—Ç—É
        self.__xml.smooth()
        Path(fname).write_text(self.__xml.prettify(), 'utf-8')

log('–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –≤ —Ä–µ–ª–∏–∑')
# ==============================================================================

release.mkdir(parents=True, exist_ok=True)

src = [i for i in first_src.glob('*') if not '.html.htm' in str(i)]
for i in src:
    if (str(i.name) == 'index.html') or (not '.html' in str(i.name)):
        shutil.copy(str(i), str(release))
    else:
        release.joinpath(f'k155{i.name}').write_bytes(i.read_bytes())

src = [i for i in second_src.glob('*') if not '.htm' in str(i) or '.html' in str(i)]
for i in src:
    shutil.copy(str(i), str(release))

for i in Path('kozak').glob('*.gif'):
    shutil.copy(str(i), str(release))
for i in [f for f in Path('kozak').glob('*.html') if not 'index' in str(f)]:
    shutil.copy(str(i), str(release))

for i in Path('libqrz').glob('*.jpg'):
    shutil.copy(str(i), str(release))
for i in Path('libqrz').glob('*.html'):
    shutil.copy(str(i), str(release))

shutil.copy('k155.djvu', str(release))
shutil.copy('k155.pdf', str(release))
shutil.copy('styles.css', str(release))

log('–°–æ–≤–º–µ—â–µ–Ω–∏–µ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏')
# ==============================================================================

import os
os.chdir(release)

first_nav = readhtml('index.html').find('nav').find('ul').find_all('li')[1:]
second_nav = readhtml('ttl.html').find('nav').find('ul').find_all('li')[1:]

scnd_nav = [i.a.get('href') for i in second_nav]

global_nav = BeautifulSoup(f'<ul></ul>', parser)

for i in first_nav:
    src, title, text = i.a.get('href'), i.a.get('title'), i.a.text.strip().replace('–ö155', '')
    i.clear()
    if Path(src).exists():
        i.append(BeautifulSoup(f'<a href="k155{src}" title="{title}">{text}</a><a href="{src}" title="{title}">{text}</a>', parser))
        if src in scnd_nav: scnd_nav.remove(src)
    else:
        i.append(BeautifulSoup(f'<a href="k155{src}" title="{title}">{text}</a><a>{text}</a>', parser))
    global_nav.ul.append(i)
    while len(scnd_nav) and not Path('k155' + scnd_nav[0]).exists():
        src = scnd_nav[0]

        if src != "ru1-3.html":
            # —Ñ–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏
            repl = {'a': '–∞', 'g': '–≥', 'p': '–ø', 'i': '–∏', 'v': '–≤', 'd': '–¥', 'e': '–µ', 'm': '–º', 'r': '—Ä', 'l': '–ª', 'n': '–Ω', 'u': '—É', 't': '—Ç', 'k': '–∫', 'x': '—Ö'}
            text = Path(src).stem
            for k, v in repl.items():
                text = text.replace(k, v)
            text = f'{text}'.upper()

            global_nav.ul.append(BeautifulSoup(f'<li><a>{text}</a><a href="{src}">{text}</a></li>', parser))
        scnd_nav.remove(src)

# –†—É—á–Ω–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–π
for i in global_nav.ul:
    if i.find_all('a')[1].text.strip() == '–ò–ï6':
        a = i.find_all('a')[1]
        a.string = '–ò–ï6-7'

    if i.find_all('a')[1].text.strip() == '–ò–ï7':
        a = i.find_all('a')[1]
        a.string = '–ò–ï6-7'
        a['href'] = 'ie6.html'
        a['title'] = str(i.a['title'])

    if i.find_all('a')[1].text.strip() == '–†–£1':
        a = i.find_all('a')[1]
        a['href'] = 'ru1-3.html'
        a['title'] = str(i.a['title'])
        a.string = '–†–£1-3'

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ kozak
for li in global_nav.find_all('li'):
    link = [i for i in li.find_all('a') if i.get('href', False)][-1]
    kz = list(Path('.').glob(f'???{link["href"].replace("k155", "")}'))
    href = str(kz[0]) if len(kz) else None
    if href:
        li.append(BeautifulSoup(f'<a href="{href}">{link.text}</a>', parser))
    else:
        li.append(BeautifulSoup(f'<a>{link.text}</a>', parser))

log ('–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ title –∏ –∫–ª–∞—Å—Å–æ–≤ —É —Å—Å—ã–ª–æ–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –º–µ–Ω—é')
# ==============================================================================
for li in global_nav.find_all('li'):
    a = li.find_all('a')
    a[0]['class'] = 'kzs'
    a[1]['class'] = 'msh'
    a[2]['class'] = 'kzk'
    for i in [i for i in a if i.get('href', None)]:
        i['title'] = ', '.join([i.text.strip() for i in readhtml(i['href']).find_all('h1')])

log('–†–µ—à–∞–µ–º –ø—Ä–æ–±–ª–µ–º—É –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∞–Ω–∞–ª–æ–≥–∞ 74170')
# ==============================================================================

altname = '74170rp1.html'
shutil.copy('74170.html', altname)
html = readhtml('rp1.html')
for a in html.find_all('a', {'href': '74170.html'}):
    a['href'] = altname
savehtml(html, 'rp1.html')

log('–ü–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏–µ –º–µ–Ω—é –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –∏ "—Ö–ª–µ–±–Ω—ã—Ö –∫—Ä–æ—à–µ–∫"')
# ==============================================================================

import copy

template = readhtml('../template.html')

global_nav.ul.insert(0, copy.copy(template.find('nav').find_all('li')[0]))
global_links = [a for a in global_nav.find_all('a') if a.get('href', 'index.html') != 'index.html']

def add_class(el, s: str):
    if isinstance(el['class'], str):
        el['class'] = [el['class']]
    el['class'].append(s)

prev_last = None
for i in global_nav.ul:
    if i.a.get('href', '') == 'index.html':
        continue

    # —Å–æ–±–∏—Ä–∞–µ–º "—Ö–ª–µ–±–Ω—ã–µ –∫—Ä–æ—à–∫–∏"
    breadcrumb = BeautifulSoup(f'<ul></ul>', parser)
    brd = None
    for j in [x for x in i.find_all('a') if x.get('href', None) != None]:
        html = readhtml(j['href'])
        if html.find('nav', {'id': 'hor'}):
            brd = html.find('nav', {'id': 'hor'}).find_all('li')[1:]
        breadcrumb.ul.extend(BeautifulSoup(f'<li><a href=\"{j["href"]}\">{j.text.strip()}</a></li>', parser))
    if brd:
        breadcrumb.ul.extend(brd)

    # –ö–ª–∞—Å—Å—ã –∏ title "—Ö–ª–µ–±–Ω—ã—Ö –∫—Ä–æ—à–µ–∫"
    for a in breadcrumb.find_all('a'):
        donor = global_nav.find('a', {'href': a['href']})
        if donor:
            a['class'] = donor['class']
            a['title'] = donor['title']
            a_end = donor
        else:
            a['class'] = ['msh', 'analog']
            a['title'] = readhtml(a['href']).find('h1').text.strip()
    # –î–æ–±–∞–≤–ª—è–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã "–≤–ø–µ—Ä–µ–¥/–Ω–∞–∑–∞–¥"
    a_end = global_links.index(global_nav.find('a', {'href': a_end['href']})) + 1
    if prev_last:
        prev_last.a.string = ''
        add_class(prev_last.a, 'prev')
        breadcrumb.ul.insert(0, prev_last)
    prev_last = copy.copy(breadcrumb.find_all('li')[-1])
    if a_end < len(global_links):
        li = BeautifulSoup().new_tag('li')
        li.insert(0, copy.copy(global_links[a_end]))
        li.a.string = ''
        add_class(li.a, 'next')
        breadcrumb.ul.insert(len(breadcrumb.find_all('li')), li)

    # –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–µ–º "–∫—Ä–æ—à–∫–∏" –∏ –±–æ–∫–æ–≤–æ–µ –º–µ–Ω—é
    for i in [i for i in breadcrumb.ul if i.a.text.strip() != '']:
        html = readhtml(i.a['href'])        
        ul = html.find('nav', {'id': 'map'}).ul
        ul.clear()
        ul.extend(copy.copy(global_nav.ul))
        hor = html.find('nav', {'id': 'hor'})
        if hor:
            hor.clear()
            hor.unwrap()
        brd = BeautifulSoup(f'<nav id="breadcrumb"></nav>', parser)
        brd.nav.append(copy.copy(breadcrumb))
        html.find('div', {'id': 'content'}).insert_before(brd)
        savehtml(html, i.a['href'])

# –ö–∞—Å—Ç–æ–º–Ω—ã–µ —Ö–ª–µ–±–Ω—ã–µ –∫—Ä–æ—à–∫–∏
brd_custom = BeautifulSoup(f'<li><a href="7493.html" class="msh analog prev"></a></li><li><a href="k155ie6.html" class="kzs">–ò–ï6</a></li><li><a href="k155ie7.html" class="kzs">–ò–ï7</a></li><li><a href="ie6.html" class="msh">–ò–ï6-7</a></li><li><a href="051ie6.html" class="kzk">–ò–ï6-7</a></li><li><a href="74192.html" class="msh analog">74192</a></li><li><a href="74193.html" class="msh analog">74193</a></li><li><a href="k155ie8.html" class="kzs next"></a></li>', parser)
for a in brd_custom.find_all('a'):
    a['title'] = ', '.join([i.text.strip() for i in readhtml(a['href']).find_all('h1')])
for i in ['k155ie6', 'k155ie7', 'ie6', '051ie6', '74192', '74193']:
    html = readhtml(f'{i}.html')
    brd = html.find('nav', {'id': 'breadcrumb'}).ul
    brd.clear()
    brd.extend(copy.copy(brd_custom))
    if i == 'ie6' or i == '051ie6':
        brd = html.find_all('nav', {'id': 'breadcrumb'})[1]
        brd.extract()
    savehtml(html, f'{i}.html')

# log('–ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –∞–Ω–∞–ª–æ–≥–æ–≤')
# ==============================================================================
# –ö–æ–¥ –∑–∞–ø—É—Å–∫–∞–ª—Å—è –æ–¥–∏–Ω —Ä–∞–∑. –í—ã—è–≤–ª–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞ —Å –∞–Ω–∞–ª–æ–≥–æ–º 74170
# err = 0
# for f in Path('.').glob('74*.html'):
#     html = readhtml(str(f))
#     if len(html.find_all('nav', {'id': 'breadcrumb'})) != 1:
#         log(f'Error! Breadcrumb in "{str(f)}" not consistent.\n')
#         err += 1
# if err > 0:
#     exit(err)

log('–†–∞–±–æ—Ç–∞ —Å –º–Ω–æ–≥–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω—ã–º —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–º')
# ==============================================================================

ttl = readhtml('5105.html').div
ttl.h1.name = 'h2'
ttl.h2.string = '–ú–∏–∫—Ä–æ—Å—Ö–µ–º—ã —Å–µ—Ä–∏–∏ –¢–¢–õ –∏ –∏—Ö –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ'
ttl.name = 'nav'
ttl['id'] = 'ttl'

log('–†–∞–±–æ—Ç–∞ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º–∏')
# ==============================================================================

# –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
for fname in ['ln4']:
    html = readhtml(str(Path('..').joinpath(f'{fname}.html')))
    tmp = copy.copy(template)
    content = tmp.find('div', {'id': 'content'})
    content.clear()
    content.extend(html.find('div', {'id': 'content'}))
    savehtml(tmp, f'{fname}.html')

def row(link: str, linktxt: str, text: str):
    return BeautifulSoup(f'<tr><td><a href="{link}">{linktxt}</a></td><td>{text}</td></tr>', parser)

# –ü–µ—Ä–µ–Ω–æ—Å–∏–º —Ç–µ–∫—Å—Ç —Å –æ—Å–Ω–æ–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ 'param.html', –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–∞–±–ª–∏—Ü—É –≤ index.html
param = readhtml('index.html')
index = copy.copy(template)
section = param.find('section', {'id': 'index'})
index.find('div', {'id': 'content'}).insert(-1, section)
savehtml(param, 'param.html')
# –ú–µ–Ω—è–µ–º —Å—Å—ã–ª–∫–∏ –≤ —Ç–∞–±–ª–∏—Ü–µ
table = section.table
for i in table.tbody.find_all('a'):
    i['href'] = f'k155{i["href"]}'
# –£–¥–∞–ª—è–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã
[br.extract() for br in table.find_all('br')]
# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –≤ —Ç–∞–±–ª–∏—Ü—É
table.tbody.insert(  8, row('iv3.html'  , '–ö155–ò–í3'  , '–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π —à–∏—Ñ—Ä–∞—Ç–æ—Ä 9 –∫–∞–Ω–∞–ª–æ–≤ –≤ 4'))
table.tbody.insert( 16, row('id7.html'  , '–ö155–ò–î7'  , '–í—ã—Å–æ–∫–æ—Å–∫–æ—Ä–æ—Å—Ç–Ω–æ–π –¥–µ—à–∏—Ñ—Ä–∞—Ç–æ—Ä'))
table.tbody.insert( 30, row('id24.html' , '–ö155–ò–î24' , '–í—ã—Å–æ–∫–æ–≤–æ–ª—å—Ç–Ω—ã–π –¥–≤–æ–∏—á–Ω–æ-–¥–µ—Å—è—Ç–∏—á–Ω—ã–π –¥–µ—à–∏—Ñ—Ä–∞—Ç–æ—Ä —Å –û–ö'))
table.tbody.insert( 62, row('ip6-7.html', '–ö155–ò–ü6-7', '4 –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö —à–∏–Ω–Ω—ã—Ö —É—Å–∏–ª–∏—Ç–µ–ª—è'))
table.tbody.insert(122, row('li4.html'  , '–ö155–õ–ò4'  , '3 –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–∞ 3–ò'))
table.tbody.insert(176, row('rp1.html'  , '–ö155–†–ü1'  , '–ú–∞—Ç—Ä–∏—Ü–∞ –û–ó–£ –Ω–∞ 16 —è—á–µ–µ–∫ (4 x 4)'))
table.tbody.insert(204, row('xl1.html'  , '–ö155–•–õ1'  , '–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç –¥–ª—è –¶–í–ú'))
# –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –≤ —Å–ª–æ–≤–∞—Ä—å
table.caption.name = 'h2'
table.insert_before(table.h2)
table.thead.extract()
table.tbody.unwrap()
for tr in table.find_all('tr'):
    dt, dd = tr.find_all('td')
    dt.name = 'dt'
    dd.name = 'dd'
    tr.unwrap()
table.name = 'dl'
# –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å—Å—ã–ª–∫–∏
for a in table.find_all('a'):
    a.string = a.text.split()[0].replace('–ö155', '').strip()
    donor = global_nav.find('a', {'href': a['href']})
    a['class'] = donor['class']
    a['title'] = donor['title']
# –î–æ–±–∞–≤–ª—è–µ–º –º–Ω–æ–≥–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω—ã–π —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫
index.find('nav', {'id': 'articles'}).insert_before(ttl)
savehtml(index, 'index.html')

# –°–æ–±–∏—Ä–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
morelist = index.find('nav', {'id': 'articles'}).find_all('a')
morelist.extend(index.find('nav', {'id': 'ttl'}).find_all('a'))
morelist.append(global_nav.a)

# –û–±–Ω–æ–≤–ª—è–µ–º –±–æ–∫–æ–≤–æ–µ –º–µ–Ω—é
for i in morelist:
    html = readhtml(i['href'])
    if not html.find('nav', {'id': 'map'}):
        html.body.insert(0, BeautifulSoup('<nav id="map"><ul></ul></nav>', parser))
    ul = html.find('nav', {'id': 'map'}).ul
    ul.clear()
    ul.extend(copy.copy(global_nav.ul))
    savehtml(html, i['href'])

log('–î–æ–±–∞–≤–ª–µ–Ω–∏–µ "—Ö–ª–µ–±–Ω—ã—Ö –∫—Ä–æ—à–µ–∫" –≤–Ω–∏–∑')
# ==============================================================================
for i in Path('.').glob('*.html'):
    html = readhtml(str(i))
    fst_brd = html.find('nav', {'id': 'breadcrumb'})
    if fst_brd:
        fst_brd['class'] = 'breadcrumb'
        scn_brd = copy.copy(fst_brd)
        del scn_brd.attrs['id']
        html.div.insert_after(scn_brd)
        savehtml(html, str(i))

log('–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã 404')
# ==============================================================================

html = readhtml('index.html')
html.h1.string = html.h1.text.strip() + ' [404]'
html.h1.insert_after(BeautifulSoup('<p>–≠—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ —Å—á–∏—Ç–∞–π—Ç–µ, —á—Ç–æ –≤—ã –Ω–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ.</p>', parser))
savehtml(html, '404.html')

log('–î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–≥ canonical –∫ –¥—É–±–ª–∏–∫–∞—Ç–∞–º')
# ==============================================================================

canonical = {
    'index'  : '404',
    '74170'  : '74170rp1',
    '037id10': '037id24',
    '114kp5' : '114kp7',
    '126pr6' : '126pr7',
    '144tm5' : '144tm7',
}
for origin, cp in canonical.items():
    for i in [origin, cp]:
        html = readhtml(f'{i}.html')
        html.head.append(BeautifulSoup(f'<link rel="canonical" href="{origin}.html">', parser))
        savehtml(html, f'{i}.html')

log('–§–∏–ª—å—Ç–∞—Ü–∏—è —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å–∞–º–æ–≥–æ —Å–µ–±—è –∏ —É–ø–∞–∫–æ–≤–∫–∞ –∫–æ–¥–∞')
# ==============================================================================

import htmlmin
import re

for i in Path('.').glob('*.html'):
    fname = str(i.name)
    html = BeautifulSoup(i.read_text(encoding = enc), parser)
    for link in html.find_all('a'):
        # –£–¥–∞–ª—è–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–∞–º–æ–≥–æ —Å–µ–±—è
        if link.get('href', '') == fname:
            del link.attrs['href']
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –¥–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
        elif '.jpg' in link.get('href', ''):
            link['style'] = f'--href:url({link["href"]})'
    html.smooth()
    # savehtml(html, fname)
    # –ö–æ–º–ø–∞–∫—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –∫–æ–¥
    html_compact = ' '.join(str(html).replace('\n', ' ').split()).replace(' </', '</')
    html_compact = html_compact.replace('<b>', '').replace('&lt;b&gt;', '') # —ç—Ç–æ –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ—Å—è–∫–∞ –≤ la3.html. –ë–æ–ª—å—à–µ –Ω–∏–≥–¥–µ –Ω–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è
    for t in ['tr', 'th', 'td', 'ul', 'li']:
        html_compact = html_compact.replace(' <' + t, '<' + t).replace(' </' + t, '</' + t).replace(t + '> ', t + '>')
    for t in ['sub', 'sup']:
        html_compact = html_compact.replace(' <' + t, '<' + t).replace(' </' + t, '</' + t).replace('<' + t + '> ', '<' + t + '>')
    for t in ['a', 'span']:
        html_compact = html_compact.replace(' </' + t, '</' + t)
        html_compact = re.sub('(<' + t + ' [^>]+>) +', r'\1', html_compact)
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–æ—á–µ—á–Ω—ã–µ –ø—Ä–∞–≤–∫–∏
    for i in ',.;:!)':
        html_compact = html_compact.replace(' ' + i, i) # –£–±—Ä–∞—Ç—å –ø—Ä–æ–±–µ–ª—ã –ø–µ—Ä–µ–¥ –∑–Ω–∞–∫–∞–º–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
    for i in '(':
        html_compact = html_compact.replace(i + ' ', i) # –£–±—Ä–∞—Ç—å –ø—Ä–æ–±–µ–ª—ã –ø–æ—Å–ª–µ –æ—Ç–∫—Ä—ã–≤–∞—é—â–∏—Ö —Å–∫–æ–±–æ–∫
    html_compact = re.sub(r'\.([–ê-–Ø])', r'. \1', html_compact) # –î–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–±–µ–ª –ø–æ—Å–ª–µ —Ç–æ—á–∫–∏
    html_compact = re.sub(r'([^l\s])(\()', r'\1 (', html_compact)  # –î–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–±–µ–ª –ø–µ—Ä–µ–¥ –æ—Ç–∫—Ä—ã–≤–∞—é—â–∏–º–∏ —Å–∫–æ–±–∫–∞–º–∏
    # —Ç–æ—á–µ—á–Ω—ã–µ –æ–ø–µ—á–∞—Ç–∫–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö
    if (fname == '74121.html'): html_compact = html_compact.replace('(74L121 ', '(74L121) ')
    html_compact = html_compact.replace('–í—Å–µ –≤—ã—Ö–æ–¥–∞ –æ—Ç–∫–ª—é—á–µ–Ω—ã', '–í—Å–µ –≤—ã—Ö–æ–¥—ã –æ—Ç–∫–ª—é—á–µ–Ω—ã')
    if (fname == 'id10.html'): html_compact = html_compact.replace('–î–µ—à–∏—Ñ—Ä–∞—Ç–æ—Ä—ã–ö155–ò–î10–∏–ö–ú155–ò–î10', '–î–µ—à–∏—Ñ—Ä–∞—Ç–æ—Ä—ã –ö155–ò–î10 –∏ –ö–ú155–ò–î10')
    if (fname == 'k155id12.html'): html_compact = html_compact.replace('–ú–∏–∫—Ä–æ—Å—Ö–µ–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç', '–ú–∏–∫—Ä–æ—Å—Ö–µ–º–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç')
    html_compact = html_compact.replace('133 –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤', '33 –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–∞')
    html_compact = html_compact.replace('–ú–∏–∫—Ä–æ—Å—Ö–µ–º—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç', '–ú–∏–∫—Ä–æ—Å—Ö–µ–º–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç')
    html_compact = html_compact.replace('–ú–∏–∫—Ä–æ—Å—Ö–µ–º–∞–ö155–ò–ï8', '–ú–∏–∫—Ä–æ—Å—Ö–µ–º–∞ –ö155–ò–ï8')
    html_compact = html_compact.replace('77 –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç', '77 –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤')
    html_compact = html_compact.replace('–¥–ª—è –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–æ-–ª–æ–≥–∏—á–µ—Å–∫–æ–µ —É—Å—Ç—Ä–æ–π—Å–≤–æ', '–¥–ª—è –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–æ-–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞')
    if (fname == 'k155ir13.html'): html_compact = html_compact.replace('—á–µ—Ç—ã—Ä–µ—Ö—Ä–∞–∑—Ä—è–¥–Ω—ã–π', '–≤–æ—Å—å–º–∏—Ä–∞–∑—Ä—è–¥–Ω—ã–π')
    if (fname == 'ld1.html'): html_compact = html_compact.replace('–ú–∏–∫—Ä–æ—Å—Ö–µ–º–∞–ö155–õ–î1', '–ú–∏–∫—Ä–æ—Å—Ö–µ–º–∞ –ö155–õ–î1')
    html_compact = html_compact.replace('‚äï', '&#8853;').replace('ü†ï', '&#8593;').replace('‚Üë', '&#8593;').replace('‚Üì', '&#8595;').replace('‚áÖ', '&#8645;').replace('‚áµ', '&#8693;')
    # –§–∏–Ω–∞–ª—å–Ω–∞—è —É–ø–∞–∫–æ–≤–∫–∞
    Path(fname).write_text(htmlmin.minify(html_compact), enc)

log('.htaccess, robots.txt –∏ sitemap.xml')
# ==============================================================================

host = 'k155.su'
Path('.htaccess').write_text('AddDefaultCharset utf-8\nErrorDocument 404 /404.html\n', 'utf-8')
Path('robots.txt').write_text('User-agent: *\n' + ''.join([f'Disallow: /{i}.html\n' for i in canonical.values()]) + f'Sitemap: https://{host}/sitemap.xml\n', 'utf-8')
sitemap = Sitemap(Path('..').joinpath('sitemap.xml'))
sitemap.extend([str(i) for i in Path('').glob('*.html')])
[sitemap.remove(f'{i}.html') for i in canonical.values()]
sitemap.save('sitemap.xml', host)

log('complete\n')
